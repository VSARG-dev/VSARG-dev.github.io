<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>V S Abhinav Rahul Gandrakota</title>
    <link>https://example.com/</link>
      <atom:link href="https://example.com/index.xml" rel="self" type="application/rss+xml" />
    <description>V S Abhinav Rahul Gandrakota</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu48098076ec83a1da7a68860ddba0d02f_33231_512x512_fill_lanczos_center_3.png</url>
      <title>V S Abhinav Rahul Gandrakota</title>
      <link>https://example.com/</link>
    </image>
    
    <item>
      <title>Example Talk</title>
      <link>https://example.com/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://example.com/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mutual Book Exchange Portal</title>
      <link>https://example.com/project/bookexchange/</link>
      <pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/bookexchange/</guid>
      <description>&lt;p&gt;As part of the Object Oriented Programming course offered by our college, we were given the task of implementing a Mutual Book Exchange Portal keeping the OOP principles in mind. The purpose of this portal was to enable the registered users to exchange the books that they have with them, free of cost, allowing for temporary use and return of books. &lt;br&gt;
&lt;br&gt;
Please check out the project Overview to get details about all the classes that were implemented, their roles, and also a site map of the final website that was hosted. Also check out our code on Github for further technical details! &lt;br&gt;
&lt;br&gt;
Website link: &lt;a href=&#34;https://bookexchangeportal.herokuapp.com/web/login&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://bookexchangeportal.herokuapp.com/web/login&lt;/a&gt; &lt;br&gt;
Admin Login Details:- &lt;br&gt;
Username: &lt;a href=&#34;mailto:admin@gmail.com&#34;&gt;admin@gmail.com&lt;/a&gt; &lt;br&gt;
Pwd: password &lt;br&gt;
&lt;br&gt;
&lt;strong&gt;NOTE:&lt;/strong&gt; Website may no longer work as Heroku has announced that they will eliminate all of their free services.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sign Language Translation</title>
      <link>https://example.com/project/sign-language-translation/</link>
      <pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/sign-language-translation/</guid>
      <description>&lt;p&gt;As part of the Machine Learning course offered by our college, we decided to implement a Sign Language Translation model keeping the hearing and speech impaired community in mind in an effort to help bridge the gap between us and them. &lt;br&gt;
&lt;br&gt;
In the past, most Sign Language Translation models have dealt with static images, wherein the model takes in an image containing a gesture being signed and outputs the &amp;ldquo;message&amp;rdquo; being contained in it. While it is definitely a solid first step, this is not at all representative of real world scenarios. &lt;br&gt;
&lt;br&gt;
In practical usage, most gestures in sign language are dynamic, i.e, the meaning is conatained in movement. Thus in essence, the problem statement then becomes the non-trivial task of interpreting text sequences from video sequences. Moreover, like other translation problems, the input word order is usually not the same as the output word order either. Both of these reasons make this a &lt;strong&gt;sequence2sequence&lt;/strong&gt; problem. &lt;br&gt;
&lt;br&gt;
We decided to start from the basics. Using &lt;a href=&#34;https://www.kaggle.com/datasets/grassknoted/asl-alphabet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; kaggle dataset, which contains ASL images labelled with their corresponding alphabet, we fine-tuned the inception-v3 model with a simple classification task. This was fairly simple, and upon achieving satisfactory results we moved on to our main objective: video processing. &lt;br&gt;
&lt;br&gt;
We then proceeded with a transformer encoder-deocder architecture. Usually in the domain of Natural Language Processing, the models almost always have a word embedding layer that serves as a look-up table for fetching the appropriate embedding vector for each word in the input. These embeddings are what are then passed through deeper layer of the encoder blocks. In our case, however, there were no words in the input. We had to somehow &amp;ldquo;embed&amp;rdquo; frames of the videos. We couldn&amp;rsquo;t use the same approach as in the case of word inputs since the image space is continuous whereas the lexicon space isn&amp;rsquo;t. In other words, there are infinitely many frames that can exist in any given video.&lt;br&gt;
&lt;br&gt;
To resolve this issue, we chose the original inception-v3 as in our previous experiment, and took the output from the penultimate layer as the embeddings. Transformers also require something called position encodings so they can keep track of the temporal ordering of frames, so they were added before passing them in. As for the specific transformer architecture itself, we used the pre-trained T5-small (small, due to GPU constraints) so that we could leverage its existing German knowledge. &lt;br&gt;
&lt;br&gt;
We tried different configurations of hyperparameters and ran training for ~1.5 hours per config. The best configuration gave us a test BLEU score of &lt;code&gt;12.75&lt;/code&gt;. While not the best, it isn&amp;rsquo;t a very representative metric as the labelled examples had only one translation per video, and BLEU&amp;rsquo;s representativity increases with more number of semantically equivalent labels. There&amp;rsquo;s also reason to believe that with a bigger transformer model and more data, the results would be much better, as is always the case with transformers. For more technical details, check out our report!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alzheimer Helper</title>
      <link>https://example.com/project/alzheimerhelper/</link>
      <pubDate>Sun, 29 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/alzheimerhelper/</guid>
      <description>&lt;p&gt;Alzheimer&amp;rsquo;s disease has become more common nowadays, affecting atleast 10% of the population above the age of 65. Families of patients suffering from Alzheimer&amp;rsquo;s have to undergo a lot of stress, having to constantly remind them of even the most essential things such as their name. This project aims to ease that burden, atleast a little, by allowing Alzheimer&amp;rsquo;s patients to converse with a chatbot and ask questions about themselves to remind themselves of the things they have forgotten. &lt;br&gt;
&lt;br&gt;
The initial page allows the family members to register and add the personal details of the patient such as their name, address, phone no., dad name, mom name, emrgency contact no., guardian name and age. A POST request is sent to the kernel which sets all the values in AIML variables. All the data is then stored in a SQLite database. &lt;br&gt;
&lt;br&gt;
Once registered, along with a username and password, users can log in and start conversing with the AI chatbot. The patient can ask simple questions about themselves such as &amp;ldquo;What is my name?&amp;rdquo;. This generates a GET request and the required details are extracted from the SQLite database following which a response is displayed from a pool of responses that have similar meaning. Finally the chats are stored in the SQLite database as well for future reference. &lt;br&gt;
&lt;br&gt;
Check out the code on Github for more details!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://example.com/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/admin/config.yml</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://example.com/publication/example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/example/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
