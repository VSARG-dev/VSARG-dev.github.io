<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | V S Abhinav Rahul Gandrakota</title>
    <link>https://example.com/project/</link>
      <atom:link href="https://example.com/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 12 Dec 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu48098076ec83a1da7a68860ddba0d02f_33231_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://example.com/project/</link>
    </image>
    
    <item>
      <title>VRFB Stack Temperature Prediction</title>
      <link>https://example.com/project/vrfb-temp-prediction/</link>
      <pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/vrfb-temp-prediction/</guid>
      <description>&lt;p&gt;Redox Flow Batteries have risen in popularity in recent years as a large-scale energy storage solution. Efficiency of the battery storage system relies on minimizing power loss, which in turn is dependent on predicting VRFB stack temperature and keeping it within a safe limit so as to prevent thermal precipitation. We have predicted variation of stack temperature with time duration of a practical 1kW 6kWh VRFB system dataset under four different operating current levels (40, 45, 50, and 60A) keeping a constant electrolyte flow rate of 180 ml/sec for both charging and discharging conditions. &lt;br&gt;
&lt;br&gt;
Here, we have used both Polynomial Regression and LSTM methods to accurately predict the stack temperature of the battery during charging and discharging profiles. The prediction accuracy of the algorithm was tested using regression metrics such as Root Mean Square Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (R2). &lt;br&gt;
&lt;br&gt;
In the case of Polynomial Regression, we created models for degrees 0-9 and implemented it on the dataset. We used gradient descent to update the weights at each iteration. The MSE error was calculated every 5000 iterations and the model was run for a total of 50000 iterations.&lt;br&gt;
&lt;br&gt;
For LSTM, We used the ‘train_test_split’ function within Scikit-learn to split the dataset 70:30 for training and testing respectively. We did not shuffle the data in this case as LSTM’s need sequential data for performing time series prediction. We reshaped the training and testing temperature data and performed Min-Max scaling using the ‘MinMaxScaler’ function within Scikit-learn so as to bring all temperature values within the range [0,1]. Min-Max scaling is performed when it is required to capture small variance in features and also for sparse data where the zero value needs to be preserved. &lt;br&gt;
&lt;br&gt;
We then used the ‘TimeseriesGenerator’ module within Tensorflow to generate a time series with number of features as 1 and number of inputs as 5. Essentially this means that the model will use 5 values from the data to predict the 6th value in the data. We then created an LSTM model having 100 hidden layers which takes an input of shape (5,1) and uses a ReLU activation function at the output layer finally giving an output of size 1. The Adam optimizer with a learning rate of 0.001, β1 = 0.9, β2 = 0.999, ϵ = 10-8 was used with no weight decay. MSE error was used as the loss function. &lt;br&gt;
&lt;br&gt;
The model was then trained on the training dataset for 50 epochs and the loss per epoch was plotted. We then used the model for prediction over the length of the entire dataset and the predicted values were stored in an array before being rescaled to the original size. The array was added later as an extra column within the dataset. The final MSE, RMSE and R2 errors were calculated and tabulated. We then plotted the algorithm performance and the parameter performance graphs. &lt;br&gt;
&lt;br&gt;
For the obtained results and graphs, check out the paper!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Audio Based Language Identification</title>
      <link>https://example.com/project/language-identification/</link>
      <pubDate>Fri, 09 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/language-identification/</guid>
      <description>&lt;p&gt;#Introduction
Language Identification (LI) is an important first step in several speech processing systems and the recommended initial step for Automatic Speech Recognition (ASR). LI is needed for ASR as ASR based systems cannot parse the uttered language in multilingual contexts causing failure in speech recognition. LI allows these systems to automatically recognize the speaker&amp;rsquo;s language and change its language settings accordingly. We propose an attention based convolutional recurrent neural network (CRNN with Attention) that works on Mel-frequency Cepstral Coefficient (MFCC) features of audio specimens. Additionally, we reproduce some state-of-the-art approaches, namely Convolutional Neural Network (CNN) and Convolutional Recurrent Neural Network (CRNN), and compare them to our proposed method. &lt;br&gt;
&lt;br&gt;
#Why Deep Learning Approach?
Over the years, studies have utilized many prosodic and acoustic features to construct machine learning models for LI systems. Every language is composed of phonemes, which are distinct unit of sounds in that language, such as &amp;lsquo;b&amp;rsquo; of black and &amp;lsquo;g&amp;rsquo; of green. Several prosodic and acoustic features are based on phonemes, which become the underlying features on whom the performance of the statistical model depends. If two languages have many overlapping phonemes, then identifying them becomes a challenging task for a classifier. Due to such drawbacks several studies have switched over to using Deep Neural Networks (DNNs) to harness their novel auto-extraction techniques. &lt;br&gt;
&lt;br&gt;
#Dataset
In the past two decades, development of LID methods has been largely fostered through NIST Language Evaluations (LREs). As a result, the most popular benchmarks for evaluating new LID models and methods are NIST LRE evaluation dataset. The NIST LREs dataset mostly contains narrow-band telephone speech. As the NIST LRE dataset is not freely available and we wanted more modern audio sources, we decided to proceed with IndicTTS dataset, which is open sourced. &lt;br&gt;
&lt;br&gt;
IndicTTS is a special corpus of Indian languages covering 13 major languages of India. These include Bengali, Hindi, Marathi, Tamil, Telugu, Assamese, Bodo(India), Gujarati, Kannada, Malayalam, Manipuri, Odia and Rajasthani. It comprises of 10000+ spoken sentences/utterances each of mono and English recorded by both Male and Female native speakers. Speech waveform files are available in .wav format along with the corresponding text. &lt;br&gt;
\&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mutual Book Exchange Portal</title>
      <link>https://example.com/project/bookexchange/</link>
      <pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/bookexchange/</guid>
      <description>&lt;p&gt;As part of the Object Oriented Programming course offered by our college, we were given the task of implementing a Mutual Book Exchange Portal keeping the OOP principles in mind. The purpose of this portal was to enable the registered users to exchange the books that they have with them, free of cost, allowing for temporary use and return of books. &lt;br&gt;
&lt;br&gt;
Please check out the project Overview to get details about all the classes that were implemented, their roles, and also a site map of the final website that was hosted. Also check out our code on Github for further technical details! &lt;br&gt;
&lt;br&gt;
Website link: &lt;a href=&#34;https://bookexchangeportal.herokuapp.com/web/login&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://bookexchangeportal.herokuapp.com/web/login&lt;/a&gt; &lt;br&gt;
Admin Login Details:- &lt;br&gt;
Username: &lt;a href=&#34;mailto:admin@gmail.com&#34;&gt;admin@gmail.com&lt;/a&gt; &lt;br&gt;
Pwd: password &lt;br&gt;
&lt;br&gt;
&lt;strong&gt;NOTE:&lt;/strong&gt; Website may no longer work as Heroku has announced that they will eliminate all of their free services.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sign Language Translation</title>
      <link>https://example.com/project/sign-language-translation/</link>
      <pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/sign-language-translation/</guid>
      <description>&lt;p&gt;As part of the Machine Learning course offered by our college, we decided to implement a Sign Language Translation model keeping the hearing and speech impaired community in mind in an effort to help bridge the gap between us and them. &lt;br&gt;
&lt;br&gt;
In the past, most Sign Language Translation models have dealt with static images, wherein the model takes in an image containing a gesture being signed and outputs the &amp;ldquo;message&amp;rdquo; being contained in it. While it is definitely a solid first step, this is not at all representative of real world scenarios. &lt;br&gt;
&lt;br&gt;
In practical usage, most gestures in sign language are dynamic, i.e, the meaning is conatained in movement. Thus in essence, the problem statement then becomes the non-trivial task of interpreting text sequences from video sequences. Moreover, like other translation problems, the input word order is usually not the same as the output word order either. Both of these reasons make this a &lt;strong&gt;sequence2sequence&lt;/strong&gt; problem. &lt;br&gt;
&lt;br&gt;
We decided to start from the basics. Using &lt;a href=&#34;https://www.kaggle.com/datasets/grassknoted/asl-alphabet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; kaggle dataset, which contains ASL images labelled with their corresponding alphabet, we fine-tuned the inception-v3 model with a simple classification task. This was fairly simple, and upon achieving satisfactory results we moved on to our main objective: video processing. &lt;br&gt;
&lt;br&gt;
We then proceeded with a transformer encoder-deocder architecture. Usually in the domain of Natural Language Processing, the models almost always have a word embedding layer that serves as a look-up table for fetching the appropriate embedding vector for each word in the input. These embeddings are what are then passed through deeper layer of the encoder blocks. In our case, however, there were no words in the input. We had to somehow &amp;ldquo;embed&amp;rdquo; frames of the videos. We couldn&amp;rsquo;t use the same approach as in the case of word inputs since the image space is continuous whereas the lexicon space isn&amp;rsquo;t. In other words, there are infinitely many frames that can exist in any given video.&lt;br&gt;
&lt;br&gt;
To resolve this issue, we chose the original inception-v3 as in our previous experiment, and took the output from the penultimate layer as the embeddings. Transformers also require something called position encodings so they can keep track of the temporal ordering of frames, so they were added before passing them in. As for the specific transformer architecture itself, we used the pre-trained T5-small (small, due to GPU constraints) so that we could leverage its existing German knowledge. &lt;br&gt;
&lt;br&gt;
We tried different configurations of hyperparameters and ran training for ~1.5 hours per config. The best configuration gave us a test BLEU score of &lt;code&gt;12.75&lt;/code&gt;. While not the best, it isn&amp;rsquo;t a very representative metric as the labelled examples had only one translation per video, and BLEU&amp;rsquo;s representativity increases with more number of semantically equivalent labels. There&amp;rsquo;s also reason to believe that with a bigger transformer model and more data, the results would be much better, as is always the case with transformers. For more technical details, check out our report!&lt;/p&gt;
&lt;!-- Credits: My teammate [Pranav Balaji](https://github.com/greenfish8090) --&gt;
</description>
    </item>
    
    <item>
      <title>Alzheimer Helper</title>
      <link>https://example.com/project/alzheimerhelper/</link>
      <pubDate>Sun, 29 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/alzheimerhelper/</guid>
      <description>&lt;p&gt;Alzheimer&amp;rsquo;s disease has become more common nowadays, affecting atleast 10% of the population above the age of 65. Families of patients suffering from Alzheimer&amp;rsquo;s have to undergo a lot of stress, having to constantly remind them of even the most essential things such as their name. This project aims to ease that burden, atleast a little, by allowing Alzheimer&amp;rsquo;s patients to converse with a chatbot and ask questions about themselves to remind themselves of the things they have forgotten. &lt;br&gt;
&lt;br&gt;
The initial page allows the family members to register and add the personal details of the patient such as their name, address, phone no., dad name, mom name, emrgency contact no., guardian name and age. A POST request is sent to the kernel which sets all the values in AIML variables. All the data is then stored in a SQLite database. &lt;br&gt;
&lt;br&gt;
Once registered, along with a username and password, users can log in and start conversing with the AI chatbot. The patient can ask simple questions about themselves such as &amp;ldquo;What is my name?&amp;rdquo;. This generates a GET request and the required details are extracted from the SQLite database following which a response is displayed from a pool of responses that have similar meaning. Finally the chats are stored in the SQLite database as well for future reference. &lt;br&gt;
&lt;br&gt;
Check out the code on Github for more details!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
