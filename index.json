
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Hi! I am V. S. Abhinav Rahul Gandrakota, a final year Electronics \u0026amp; Instrumentation undergraduate at BITS Pilani, Hyderabad. I am mainly interested in the field of data science, especially Natural Language Processing and Artificial Intelligence. Previously, I worked with Dr. Ankur Bhattacharjee on using Deep Learning based algorithms for development and optimization of an efficient VRFB thermal management system. I am currently working with Dr. Aruna Malapati on getting pre-trained embeddings for code-mixed data. I got my first opportunity to work in the Deep Learning domain as part of my summer internship at North Eastern Space Application Centre, which involved 3D Semantic Segmentation task performed on satellite generated images. After that, I knew that I wanted to pursue my professional career in this domain. I’ve worked on many assignments and projects as part of the courses done under the data science minor. Some of them include Sign Language Translation, Audio Based Language Identiifcation and Alzheimer Helper. I’ve also worked on Robotics projects in the past, first in 10th grade, and then in my first year of college. Although I don’t enjoy the theoretical aspect of electronics, I have always found working with actual hardware a lot of fun. I hope to get the opportunity to incorporate my knowledge of AI/ML along with Robotics in the future.\nI have always loved the subject of Mathematics, particularly Probability \u0026amp; Statistics. That was what, despite being an Electronics major, initially pushed me to explore the field of data science. In my free time, I like to read books, play football and work out. I’ve also recently started learning how to play the guitar. I hope I can learn how to play my favourite songs very soon! Feel free to contact me if you’d like to know more about my work!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hi! I am V. S. Abhinav Rahul Gandrakota, a final year Electronics \u0026 Instrumentation undergraduate at BITS Pilani, Hyderabad. I am mainly interested in the field of data science, especially Natural Language Processing and Artificial Intelligence.","tags":null,"title":"V S Abhinav Rahul Gandrakota","type":"authors"},{"authors":null,"categories":null,"content":"Redox Flow Batteries have risen in popularity in recent years as a large-scale energy storage solution. Efficiency of the battery storage system relies on minimizing power loss, which in turn is dependent on predicting VRFB stack temperature and keeping it within a safe limit so as to prevent thermal precipitation. We have predicted variation of stack temperature with time duration of a practical 1kW 6kWh VRFB system dataset under four different operating current levels (40, 45, 50, and 60A) keeping a constant electrolyte flow rate of 180 ml/sec for both charging and discharging conditions. Here, we have used both Polynomial Regression and LSTM methods to accurately predict the stack temperature of the battery during charging and discharging profiles. The prediction accuracy of the algorithm was tested using regression metrics such as Root Mean Square Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (R2). In the case of Polynomial Regression, we created models for degrees 0-9 and implemented it on the dataset. We used gradient descent to update the weights at each iteration. The MSE error was calculated every 5000 iterations and the model was run for a total of 50000 iterations.\nFor LSTM, We used the ‘train_test_split’ function within Scikit-learn to split the dataset 70:30 for training and testing respectively. We did not shuffle the data in this case as LSTM’s need sequential data for performing time series prediction. We reshaped the training and testing temperature data and performed Min-Max scaling using the ‘MinMaxScaler’ function within Scikit-learn so as to bring all temperature values within the range [0,1]. Min-Max scaling is performed when it is required to capture small variance in features and also for sparse data where the zero value needs to be preserved. We then used the ‘TimeseriesGenerator’ module within Tensorflow to generate a time series with number of features as 1 and number of inputs as 5. Essentially this means that the model will use 5 values from the data to predict the 6th value in the data. We then created an LSTM model having 100 hidden layers which takes an input of shape (5,1) and uses a ReLU activation function at the output layer finally giving an output of size 1. The Adam optimizer with a learning rate of 0.001, β1 = 0.9, β2 = 0.999, ϵ = 10-8 was used with no weight decay. MSE error was used as the loss function. The model was then trained on the training dataset for 50 epochs and the loss per epoch was plotted. We then used the model for prediction over the length of the entire dataset and the predicted values were stored in an array before being rescaled to the original size. The array was added later as an extra column within the dataset. The final MSE, RMSE and R2 errors were calculated and tabulated. We then plotted the algorithm performance and the parameter performance graphs. For the obtained results and graphs, check out the paper!\n","date":1670803200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670803200,"objectID":"668c73f258a2db67d7b650c34f519481","permalink":"https://example.com/project/vrfb-temp-prediction/","publishdate":"2022-12-12T00:00:00Z","relpermalink":"/project/vrfb-temp-prediction/","section":"project","summary":"Predicted variation of Vanadium Redox Flow Battery stack temperature with time using Polynomial Regression and LSTM.","tags":["Deep Learning","Tensorflow","LSTM","Polynomial Regression"],"title":"VRFB Stack Temperature Prediction","type":"project"},{"authors":null,"categories":null,"content":"Introduction Language Identification (LI) is an important first step in several speech processing systems and the recommended initial step for Automatic Speech Recognition (ASR). LI is needed for ASR as ASR based systems cannot parse the uttered language in multilingual contexts causing failure in speech recognition. LI allows these systems to automatically recognize the speaker’s language and change its language settings accordingly. We propose an attention based convolutional recurrent neural network (CRNN with Attention) that works on Mel-frequency Cepstral Coefficient (MFCC) features of audio specimens. Additionally, we reproduce some state-of-the-art approaches, namely Convolutional Neural Network (CNN) and Convolutional Recurrent Neural Network (CRNN), and compare them to our proposed method.\nOverview Why Deep Learning Approach? Over the years, studies have utilized many prosodic and acoustic features to construct machine learning models for LI systems. Every language is composed of phonemes, which are distinct unit of sounds in that language, such as ‘b’ of black and ‘g’ of green. Several prosodic and acoustic features are based on phonemes, which become the underlying features on whom the performance of the statistical model depends. If two languages have many overlapping phonemes, then identifying them becomes a challenging task for a classifier. Due to such drawbacks several studies have switched over to using Deep Neural Networks (DNNs) to harness their novel auto-extraction techniques.\nDataset In the past two decades, development of LID methods has been largely fostered through NIST Language Evaluations (LREs). As a result, the most popular benchmarks for evaluating new LID models and methods are NIST LRE evaluation dataset. The NIST LREs dataset mostly contains narrow-band telephone speech. As the NIST LRE dataset is not freely available and we wanted more modern audio sources, we decided to proceed with IndicTTS dataset, which is open sourced. IndicTTS is a special corpus of Indian languages covering 13 major languages of India. These include Bengali, Hindi, Marathi, Tamil, Telugu, Assamese, Bodo(India), Gujarati, Kannada, Malayalam, Manipuri, Odia and Rajasthani. It comprises of 10000+ spoken sentences/utterances each of mono and English recorded by both Male and Female native speakers. Speech waveform files are available in .wav format along with the corresponding text.\nPreprocessing To make our gathered data compatible with our LID system, we need to do some preprocessing. As a first step, we encode all audio files in the uncompressed, lossless WAVE format, as this format allows for future manipulations without any deterioration in signal quality. As phonemes in most of the Indian languages do not exceed 3 kHz in conversational speech, we only include frequencies of up to 5 kHz. We then extracted the features using Mel Frequency Cepstral Coefficients (MFCC).\nSteps for MFCC feature extraction For more information, check out the code on Github!\n","date":1670544000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670544000,"objectID":"e8d7e96aeaf7013a455cf15a83e55da9","permalink":"https://example.com/project/language-identification/","publishdate":"2022-12-09T00:00:00Z","relpermalink":"/project/language-identification/","section":"project","summary":"Given an audio input, identified the language that was spoken using CNNs, CRNNs and Attention-based CRNNs.","tags":["Deep Learning","Tensorflow","Sklearn","NLP","Librosa"],"title":"Audio Based Language Identification","type":"project"},{"authors":null,"categories":null,"content":"As part of the Object Oriented Programming course offered by our college, we were given the task of implementing a Mutual Book Exchange Portal keeping the OOP principles in mind. The purpose of this portal was to enable the registered users to exchange the books that they have with them, free of cost, allowing for temporary use and return of books. Please check out the project Overview to get details about all the classes that were implemented, their roles, and also a site map of the final website that was hosted. Also check out our code on Github for further technical details! Website link: https://bookexchangeportal.herokuapp.com/web/login Admin Login Details:- Username: admin@gmail.com Pwd: password NOTE: Website may no longer work as Heroku has announced that they will eliminate all of their free services.\n","date":1651017600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651017600,"objectID":"5759483e7e0897ccfc2bd0b8147a5e8e","permalink":"https://example.com/project/bookexchange/","publishdate":"2022-04-27T00:00:00Z","relpermalink":"/project/bookexchange/","section":"project","summary":"Web-based Application that enables the registered users to exchange books they have with them, free of cost, for the purpose of temporary use, and return.","tags":["Full Stack Web Dev","SpringBoot","MySQL","Heroku"],"title":"Mutual Book Exchange Portal","type":"project"},{"authors":null,"categories":null,"content":"As part of the Machine Learning course offered by our college, we decided to implement a Sign Language Translation model keeping the hearing and speech impaired community in mind in an effort to help bridge the gap between us and them. In the past, most Sign Language Translation models have dealt with static images, wherein the model takes in an image containing a gesture being signed and outputs the “message” being contained in it. While it is definitely a solid first step, this is not at all representative of real world scenarios. In practical usage, most gestures in sign language are dynamic, i.e, the meaning is conatained in movement. Thus in essence, the problem statement then becomes the non-trivial task of interpreting text sequences from video sequences. Moreover, like other translation problems, the input word order is usually not the same as the output word order either. Both of these reasons make this a sequence2sequence problem. We decided to start from the basics. Using this kaggle dataset, which contains ASL images labelled with their corresponding alphabet, we fine-tuned the inception-v3 model with a simple classification task. This was fairly simple, and upon achieving satisfactory results we moved on to our main objective: video processing. We then proceeded with a transformer encoder-deocder architecture. Usually in the domain of Natural Language Processing, the models almost always have a word embedding layer that serves as a look-up table for fetching the appropriate embedding vector for each word in the input. These embeddings are what are then passed through deeper layer of the encoder blocks. In our case, however, there were no words in the input. We had to somehow “embed” frames of the videos. We couldn’t use the same approach as in the case of word inputs since the image space is continuous whereas the lexicon space isn’t. In other words, there are infinitely many frames that can exist in any given video.\nTo resolve this issue, we chose the original inception-v3 as in our previous experiment, and took the output from the penultimate layer as the embeddings. Transformers also require something called position encodings so they can keep track of the temporal ordering of frames, so they were added before passing them in. As for the specific transformer architecture itself, we used the pre-trained T5-small (small, due to GPU constraints) so that we could leverage its existing German knowledge. We tried different configurations of hyperparameters and ran training for ~1.5 hours per config. The best configuration gave us a test BLEU score of 12.75. While not the best, it isn’t a very representative metric as the labelled examples had only one translation per video, and BLEU’s representativity increases with more number of semantically equivalent labels. There’s also reason to believe that with a bigger transformer model and more data, the results would be much better, as is always the case with transformers. For more technical details, check out our report!\n","date":1650412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650412800,"objectID":"9937e3cdad3be8f4707e9bec08c607eb","permalink":"https://example.com/project/sign-language-translation/","publishdate":"2022-04-20T00:00:00Z","relpermalink":"/project/sign-language-translation/","section":"project","summary":"A transformer model that takes in video sequences signed in German Sign Language and translates them into readable German text.","tags":["Deep Learning","Sequence Processing","Video Processing","Machine Translation","Natural Language Generation"],"title":"Sign Language Translation","type":"project"},{"authors":null,"categories":null,"content":"Alzheimer’s disease has become more common nowadays, affecting atleast 10% of the population above the age of 65. Families of patients suffering from Alzheimer’s have to undergo a lot of stress, having to constantly remind them of even the most essential things such as their name. This project aims to ease that burden, atleast a little, by allowing Alzheimer’s patients to converse with a chatbot and ask questions about themselves to remind themselves of the things they have forgotten. The initial page allows the family members to register and add the personal details of the patient such as their name, address, phone no., dad name, mom name, emrgency contact no., guardian name and age. A POST request is sent to the kernel which sets all the values in AIML variables. All the data is then stored in a SQLite database. Once registered, along with a username and password, users can log in and start conversing with the AI chatbot. The patient can ask simple questions about themselves such as “What is my name?”. This generates a GET request and the required details are extracted from the SQLite database following which a response is displayed from a pool of responses that have similar meaning. Finally the chats are stored in the SQLite database as well for future reference. Check out the code on Github for more details!\n","date":1606608000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606608000,"objectID":"f61a5cd98fe402497340b55bcd065b51","permalink":"https://example.com/project/alzheimerhelper/","publishdate":"2020-11-29T00:00:00Z","relpermalink":"/project/alzheimerhelper/","section":"project","summary":"An AI chatbot to help people suffering from Alzheimer's.","tags":["Chatbot","Flask","AIML","SQLite"],"title":"Alzheimer Helper","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://example.com/talk/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talk/","section":"event","summary":"","tags":null,"title":"","type":"event"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://example.com/privacy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/privacy/","section":"","summary":"","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ff6a19061a984819d30c916886db56ef","permalink":"https://example.com/publication/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/example/","section":"publication","summary":"","tags":null,"title":"","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9b10c1f64082d3869fd4cb1f85809430","permalink":"https://example.com/terms/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/terms/","section":"","summary":"","tags":null,"title":"","type":"page"}]