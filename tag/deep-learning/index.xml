<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | V S Abhinav Rahul Gandrakota</title>
    <link>https://example.com/tag/deep-learning/</link>
      <atom:link href="https://example.com/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 12 Dec 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu48098076ec83a1da7a68860ddba0d02f_33231_512x512_fill_lanczos_center_3.png</url>
      <title>Deep Learning</title>
      <link>https://example.com/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>VRFB Stack Temperature Prediction</title>
      <link>https://example.com/project/vrfb-temp-prediction/</link>
      <pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/vrfb-temp-prediction/</guid>
      <description>&lt;p&gt;Redox Flow Batteries have risen in popularity in recent years as a large-scale energy storage solution. Efficiency of the battery storage system relies on minimizing power loss, which in turn is dependent on predicting VRFB stack temperature and keeping it within a safe limit so as to prevent thermal precipitation. We have predicted variation of stack temperature with time duration of a practical 1kW 6kWh VRFB system dataset under four different operating current levels (40, 45, 50, and 60A) keeping a constant electrolyte flow rate of 180 ml/sec for both charging and discharging conditions. &lt;br&gt;
&lt;br&gt;
Here, we have used both Polynomial Regression and LSTM methods to accurately predict the stack temperature of the battery during charging and discharging profiles. The prediction accuracy of the algorithm was tested using regression metrics such as Root Mean Square Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (R2). &lt;br&gt;
&lt;br&gt;
In the case of Polynomial Regression, we created models for degrees 0-9 and implemented it on the dataset. We used gradient descent to update the weights at each iteration. The MSE error was calculated every 5000 iterations and the model was run for a total of 50000 iterations.&lt;br&gt;
&lt;br&gt;
For LSTM, We used the ‘train_test_split’ function within Scikit-learn to split the dataset 70:30 for training and testing respectively. We did not shuffle the data in this case as LSTM’s need sequential data for performing time series prediction. We reshaped the training and testing temperature data and performed Min-Max scaling using the ‘MinMaxScaler’ function within Scikit-learn so as to bring all temperature values within the range [0,1]. Min-Max scaling is performed when it is required to capture small variance in features and also for sparse data where the zero value needs to be preserved. &lt;br&gt;
&lt;br&gt;
We then used the ‘TimeseriesGenerator’ module within Tensorflow to generate a time series with number of features as 1 and number of inputs as 5. Essentially this means that the model will use 5 values from the data to predict the 6th value in the data. We then created an LSTM model having 100 hidden layers which takes an input of shape (5,1) and uses a ReLU activation function at the output layer finally giving an output of size 1. The Adam optimizer with a learning rate of 0.001, β1 = 0.9, β2 = 0.999, ϵ = 10-8 was used with no weight decay. MSE error was used as the loss function. &lt;br&gt;
&lt;br&gt;
The model was then trained on the training dataset for 50 epochs and the loss per epoch was plotted. We then used the model for prediction over the length of the entire dataset and the predicted values were stored in an array before being rescaled to the original size. The array was added later as an extra column within the dataset. The final MSE, RMSE and R2 errors were calculated and tabulated. We then plotted the algorithm performance and the parameter performance graphs. &lt;br&gt;
&lt;br&gt;
For the obtained results and graphs, check out the paper!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Audio Based Language Identification</title>
      <link>https://example.com/project/language-identification/</link>
      <pubDate>Fri, 09 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/language-identification/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Language Identification (LI) is an important first step in several speech processing systems and the recommended initial step for Automatic Speech Recognition (ASR). LI is needed for ASR as ASR based systems cannot parse the uttered language in multilingual contexts causing failure in speech recognition. LI allows these systems to automatically recognize the speaker&amp;rsquo;s language and change its language settings accordingly. We propose an attention based convolutional recurrent neural network (CRNN with Attention) that works on Mel-frequency Cepstral Coefficient (MFCC) features of audio specimens. Additionally, we reproduce some state-of-the-art approaches, namely Convolutional Neural Network (CNN) and Convolutional Recurrent Neural Network (CRNN), and compare them to our proposed method.&lt;/p&gt;
















&lt;figure  id=&#34;figure-overview&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Overview&#34; srcset=&#34;
               /project/language-identification/images/lid_preprocess_huc278c2f378d7c227ea87461a771fda85_5143_7fc69ac80f5725384f6d887a9dc5f053.webp 400w,
               /project/language-identification/images/lid_preprocess_huc278c2f378d7c227ea87461a771fda85_5143_2f936fffdefe8ee40bf904f59eca06d6.webp 760w,
               /project/language-identification/images/lid_preprocess_huc278c2f378d7c227ea87461a771fda85_5143_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://example.com/project/language-identification/images/lid_preprocess_huc278c2f378d7c227ea87461a771fda85_5143_7fc69ac80f5725384f6d887a9dc5f053.webp&#34;
               width=&#34;314&#34;
               height=&#34;161&#34;
               loading=&#34;lazy&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Overview
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;why-deep-learning-approach&#34;&gt;Why Deep Learning Approach?&lt;/h2&gt;
&lt;p&gt;Over the years, studies have utilized many prosodic and acoustic features to construct machine learning models for LI systems. Every language is composed of phonemes, which are distinct unit of sounds in that language, such as &amp;lsquo;b&amp;rsquo; of black and &amp;lsquo;g&amp;rsquo; of green. Several prosodic and acoustic features are based on phonemes, which become the underlying features on whom the performance of the statistical model depends. If two languages have many overlapping phonemes, then identifying them becomes a challenging task for a classifier. Due to such drawbacks several studies have switched over to using Deep Neural Networks (DNNs) to harness their novel auto-extraction techniques.&lt;/p&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;In the past two decades, development of LID methods has been largely fostered through NIST Language Evaluations (LREs). As a result, the most popular benchmarks for evaluating new LID models and methods are NIST LRE evaluation dataset. The NIST LREs dataset mostly contains narrow-band telephone speech. As the NIST LRE dataset is not freely available and we wanted more modern audio sources, we decided to proceed with IndicTTS dataset, which is open sourced. &lt;br&gt;
&lt;br&gt;
IndicTTS is a special corpus of Indian languages covering 13 major languages of India. These include Bengali, Hindi, Marathi, Tamil, Telugu, Assamese, Bodo(India), Gujarati, Kannada, Malayalam, Manipuri, Odia and Rajasthani. It comprises of 10000+ spoken sentences/utterances each of mono and English recorded by both Male and Female native speakers. Speech waveform files are available in .wav format along with the corresponding text.&lt;/p&gt;
&lt;h2 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h2&gt;
&lt;p&gt;To make our gathered data compatible with our LID system, we need to do some preprocessing. As a first step, we encode all audio files in the uncompressed, lossless WAVE format, as this format allows for future manipulations without any deterioration in signal quality. As phonemes in most of the Indian languages do not exceed 3 kHz in conversational speech, we only include frequencies of up to 5 kHz. We then extracted the features using Mel Frequency Cepstral Coefficients (MFCC).&lt;/p&gt;
















&lt;figure  id=&#34;figure-steps-for-mfcc-feature-extraction&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Steps for MFCC feature extraction&#34; srcset=&#34;
               /project/language-identification/images/lid_mfcc_hub9d38341259648edf8ef05264acc042b_57068_add7b267b1d5e683f921ca30cab3eb7c.webp 400w,
               /project/language-identification/images/lid_mfcc_hub9d38341259648edf8ef05264acc042b_57068_b3122368d4f20214b47dfe301c00406e.webp 760w,
               /project/language-identification/images/lid_mfcc_hub9d38341259648edf8ef05264acc042b_57068_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://example.com/project/language-identification/images/lid_mfcc_hub9d38341259648edf8ef05264acc042b_57068_add7b267b1d5e683f921ca30cab3eb7c.webp&#34;
               width=&#34;361&#34;
               height=&#34;666&#34;
               loading=&#34;lazy&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Steps for MFCC feature extraction
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;For more information, check out the code on Github!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sign Language Translation</title>
      <link>https://example.com/project/sign-language-translation/</link>
      <pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/sign-language-translation/</guid>
      <description>&lt;p&gt;As part of the Machine Learning course offered by our college, we decided to implement a Sign Language Translation model keeping the hearing and speech impaired community in mind in an effort to help bridge the gap between us and them. &lt;br&gt;
&lt;br&gt;
In the past, most Sign Language Translation models have dealt with static images, wherein the model takes in an image containing a gesture being signed and outputs the &amp;ldquo;message&amp;rdquo; being contained in it. While it is definitely a solid first step, this is not at all representative of real world scenarios. &lt;br&gt;
&lt;br&gt;
In practical usage, most gestures in sign language are dynamic, i.e, the meaning is conatained in movement. Thus in essence, the problem statement then becomes the non-trivial task of interpreting text sequences from video sequences. Moreover, like other translation problems, the input word order is usually not the same as the output word order either. Both of these reasons make this a &lt;strong&gt;sequence2sequence&lt;/strong&gt; problem. &lt;br&gt;
&lt;br&gt;
We decided to start from the basics. Using &lt;a href=&#34;https://www.kaggle.com/datasets/grassknoted/asl-alphabet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; kaggle dataset, which contains ASL images labelled with their corresponding alphabet, we fine-tuned the inception-v3 model with a simple classification task. This was fairly simple, and upon achieving satisfactory results we moved on to our main objective: video processing. &lt;br&gt;
&lt;br&gt;
We then proceeded with a transformer encoder-deocder architecture. Usually in the domain of Natural Language Processing, the models almost always have a word embedding layer that serves as a look-up table for fetching the appropriate embedding vector for each word in the input. These embeddings are what are then passed through deeper layer of the encoder blocks. In our case, however, there were no words in the input. We had to somehow &amp;ldquo;embed&amp;rdquo; frames of the videos. We couldn&amp;rsquo;t use the same approach as in the case of word inputs since the image space is continuous whereas the lexicon space isn&amp;rsquo;t. In other words, there are infinitely many frames that can exist in any given video.&lt;br&gt;
&lt;br&gt;
To resolve this issue, we chose the original inception-v3 as in our previous experiment, and took the output from the penultimate layer as the embeddings. Transformers also require something called position encodings so they can keep track of the temporal ordering of frames, so they were added before passing them in. As for the specific transformer architecture itself, we used the pre-trained T5-small (small, due to GPU constraints) so that we could leverage its existing German knowledge. &lt;br&gt;
&lt;br&gt;
We tried different configurations of hyperparameters and ran training for ~1.5 hours per config. The best configuration gave us a test BLEU score of &lt;code&gt;12.75&lt;/code&gt;. While not the best, it isn&amp;rsquo;t a very representative metric as the labelled examples had only one translation per video, and BLEU&amp;rsquo;s representativity increases with more number of semantically equivalent labels. There&amp;rsquo;s also reason to believe that with a bigger transformer model and more data, the results would be much better, as is always the case with transformers. For more technical details, check out our report!&lt;/p&gt;
&lt;!-- Credits: My teammate [Pranav Balaji](https://github.com/greenfish8090) --&gt;
</description>
    </item>
    
  </channel>
</rss>
